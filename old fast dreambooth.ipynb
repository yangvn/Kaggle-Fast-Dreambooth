{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-26T10:11:10.664820Z","iopub.execute_input":"2022-12-26T10:11:10.665944Z","iopub.status.idle":"2022-12-26T10:11:10.765772Z","shell.execute_reply.started":"2022-12-26T10:11:10.665825Z","shell.execute_reply":"2022-12-26T10:11:10.764734Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/imgdata/imgdata/sdttdt108.jpg\n/kaggle/input/imgdata/imgdata/sdttdt21.jpg\n/kaggle/input/imgdata/imgdata/sdttdt51.jpg\n/kaggle/input/imgdata/imgdata/sdttdt68.jpg\n/kaggle/input/imgdata/imgdata/sdttdt104.jpg\n/kaggle/input/imgdata/imgdata/sdttdt72.jpg\n/kaggle/input/imgdata/imgdata/sdttdt04.jpg\n/kaggle/input/imgdata/imgdata/sdttdt40.jpg\n/kaggle/input/imgdata/imgdata/sdttdt98.jpg\n/kaggle/input/imgdata/imgdata/sdttdt84.jpg\n/kaggle/input/imgdata/imgdata/sdttdt15.jpg\n/kaggle/input/imgdata/imgdata/sdttdt20.jpg\n/kaggle/input/imgdata/imgdata/sdttdt112.jpg\n/kaggle/input/imgdata/imgdata/sdttdt31.jpg\n/kaggle/input/imgdata/imgdata/sdttdt103.jpg\n/kaggle/input/imgdata/imgdata/sdttdt92.jpg\n/kaggle/input/imgdata/imgdata/sdttdt69.jpg\n/kaggle/input/imgdata/imgdata/sdttdt47.jpg\n/kaggle/input/imgdata/imgdata/sdttdt12.jpg\n/kaggle/input/imgdata/imgdata/sdttdt87.jpg\n/kaggle/input/imgdata/imgdata/sdttdt38.jpg\n/kaggle/input/imgdata/imgdata/sdttdt28.jpg\n/kaggle/input/imgdata/imgdata/sdttdt17.jpg\n/kaggle/input/imgdata/imgdata/sdttdt03.jpg\n/kaggle/input/imgdata/imgdata/sdttdt02.jpg\n/kaggle/input/imgdata/imgdata/sdttdt46.jpg\n/kaggle/input/imgdata/imgdata/sdttdt26.jpg\n/kaggle/input/imgdata/imgdata/sdttdt25.jpg\n/kaggle/input/imgdata/imgdata/sdttdt107.jpg\n/kaggle/input/imgdata/imgdata/sdttdt89.jpg\n/kaggle/input/imgdata/imgdata/sdttdt07.jpg\n/kaggle/input/imgdata/imgdata/sdttdt24.jpg\n/kaggle/input/imgdata/imgdata/sdttdt91.jpg\n/kaggle/input/imgdata/imgdata/sdttdt99.jpg\n/kaggle/input/imgdata/imgdata/sdttdt58.jpg\n/kaggle/input/imgdata/imgdata/sdttdt96.jpg\n/kaggle/input/imgdata/imgdata/sdttdt82.jpg\n/kaggle/input/imgdata/imgdata/sdttdt33.jpg\n/kaggle/input/imgdata/imgdata/sdttdt30.jpg\n/kaggle/input/imgdata/imgdata/sdttdt22.jpg\n/kaggle/input/imgdata/imgdata/sdttdt42.jpg\n/kaggle/input/imgdata/imgdata/sdttdt78.jpg\n/kaggle/input/imgdata/imgdata/sdttdt54.jpg\n/kaggle/input/imgdata/imgdata/sdttdt56.jpg\n/kaggle/input/imgdata/imgdata/sdttdt102.jpg\n/kaggle/input/imgdata/imgdata/sdttdt53.jpg\n/kaggle/input/imgdata/imgdata/sdttdt52.jpg\n/kaggle/input/imgdata/imgdata/sdttdt18.jpg\n/kaggle/input/imgdata/imgdata/sdttdt41.jpg\n/kaggle/input/imgdata/imgdata/sdttdt27.jpg\n/kaggle/input/imgdata/imgdata/sdttdt70.jpg\n/kaggle/input/imgdata/imgdata/sdttdt43.jpg\n/kaggle/input/imgdata/imgdata/sdttdt39.jpg\n/kaggle/input/imgdata/imgdata/sdttdt61.jpg\n/kaggle/input/imgdata/imgdata/sdttdt81.jpg\n/kaggle/input/imgdata/imgdata/sdttdt75.jpg\n/kaggle/input/imgdata/imgdata/sdttdt13.jpg\n/kaggle/input/imgdata/imgdata/sdttdt100.jpg\n/kaggle/input/imgdata/imgdata/sdttdt57.jpg\n/kaggle/input/imgdata/imgdata/sdttdt90.jpg\n/kaggle/input/imgdata/imgdata/sdttdt11.jpg\n/kaggle/input/imgdata/imgdata/sdttdt71.jpg\n/kaggle/input/imgdata/imgdata/sdttdt101.jpg\n/kaggle/input/imgdata/imgdata/sdttdt45.jpg\n/kaggle/input/imgdata/imgdata/sdttdt63.jpg\n/kaggle/input/imgdata/imgdata/sdttdt10.jpg\n/kaggle/input/imgdata/imgdata/sdttdt74.jpg\n/kaggle/input/imgdata/imgdata/sdttdt29.jpg\n/kaggle/input/imgdata/imgdata/sdttdt76.jpg\n/kaggle/input/imgdata/imgdata/sdttdt66.jpg\n/kaggle/input/imgdata/imgdata/sdttdt48.jpg\n/kaggle/input/imgdata/imgdata/sdttdt83.jpg\n/kaggle/input/imgdata/imgdata/sdttdt67.jpg\n/kaggle/input/imgdata/imgdata/sdttdt64.jpg\n/kaggle/input/imgdata/imgdata/sdttdt08.jpg\n/kaggle/input/imgdata/imgdata/sdttdt34.jpg\n/kaggle/input/imgdata/imgdata/sdttdt05.jpg\n/kaggle/input/imgdata/imgdata/sdttdt09.jpg\n/kaggle/input/imgdata/imgdata/sdttdt106.jpg\n/kaggle/input/imgdata/imgdata/sdttdt95.jpg\n/kaggle/input/imgdata/imgdata/sdttdt111.jpg\n/kaggle/input/imgdata/imgdata/sdttdt86.jpg\n/kaggle/input/imgdata/imgdata/sdttdt35.jpg\n/kaggle/input/imgdata/imgdata/sdttdt105.jpg\n/kaggle/input/imgdata/imgdata/sdttdt77.jpg\n/kaggle/input/imgdata/imgdata/sdttdt93.jpg\n/kaggle/input/imgdata/imgdata/sdttdt88.jpg\n/kaggle/input/imgdata/imgdata/sdttdt16.jpg\n/kaggle/input/imgdata/imgdata/sdttdt109.jpg\n/kaggle/input/imgdata/imgdata/sdttdt62.jpg\n/kaggle/input/imgdata/imgdata/sdttdt37.jpg\n/kaggle/input/imgdata/imgdata/sdttdt19.jpg\n/kaggle/input/imgdata/imgdata/sdttdt44.jpg\n/kaggle/input/imgdata/imgdata/sdttdt59.jpg\n/kaggle/input/imgdata/imgdata/sdttdt94.jpg\n/kaggle/input/imgdata/imgdata/sdttdt73.jpg\n/kaggle/input/imgdata/imgdata/sdttdt110.jpg\n/kaggle/input/imgdata/imgdata/sdttdt79.jpg\n/kaggle/input/imgdata/imgdata/sdttdt36.jpg\n/kaggle/input/imgdata/imgdata/sdttdt97.jpg\n/kaggle/input/imgdata/imgdata/sdttdt50.jpg\n/kaggle/input/imgdata/imgdata/sdttdt65.jpg\n/kaggle/input/imgdata/imgdata/sdttdt01.jpg\n/kaggle/input/imgdata/imgdata/sdttdt85.jpg\n/kaggle/input/imgdata/imgdata/sdttdt55.jpg\n/kaggle/input/imgdata/imgdata/sdttdt80.jpg\n/kaggle/input/imgdata/imgdata/sdttdt49.jpg\n/kaggle/input/imgdata/imgdata/sdttdt06.jpg\n/kaggle/input/imgdata/imgdata/sdttdt60.jpg\n/kaggle/input/imgdata/imgdata/sdttdt14.jpg\n/kaggle/input/imgdata/imgdata/sdttdt23.jpg\n/kaggle/input/imgdata/imgdata/sdttdt32.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"Model_Version = \"V2.1-768px\" #@param [ \"1.5\", \"V2.1-512px\", \"V2.1-768px\"]\nHuggingface_Token = \"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\" #@param {type:\"string\"}\nSession_Name = \"sketchart5\" #@param{type: 'string'}\nIMAGES_FOLDER_OPTIONAL=\"/kaggle/input/imgdata/imgdata\"\nCrop_images= True #@param{type: 'boolean'}\nCrop_size = \"768\" #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"]\nResume_Training = True #@param {type:\"boolean\"}\nTraining_Steps=11000 #@param{type: 'number'}\n#@markdown - Total Steps = Number of Instance images * 200, if you use 30 images, use 6000 steps, if you're not satisfied with the result, resume training for another 500 steps, and so on ...\nResolution = \"768\" #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"]\nfp16 = True #@param {type:\"boolean\"}\nEnable_text_encoder_training= True #@param{type: 'boolean'}\n#@markdown - At least 10% of the total training steps are needed, it doesn't matter if they are at the beginning or in the middle or the end, in case you're training the model multiple times.\n#@markdown - For example you can devide 5%, 5%, 5% on 3 training runs on the model, or 0%, 0%, 15%, given that 15% will cover the total training steps count (15% of 200 steps is not enough).\n#@markdown - Enter the % of the total steps for which to train the text_encoder\nTrain_text_encoder_for=5 #@param{type: 'number'}\n#@markdown - If you're training a style, keep it between 10-20%, if you're training on a person, set it between 50-70%, reduce it if you can't stylize the person/object.\n#@markdown - Higher % will give more weight to the instance, it gives stronger results at lower steps count, but harder to stylize.\nSave_Checkpoint_Every_n_Steps = False #@param {type:\"boolean\"}\nSave_Checkpoint_Every=1000 #@param{type: 'number'}\nStart_saving_from_the_step=6000 #@param{type: 'number'}\nDisconnect_after_training=False #@param {type:\"boolean\"}\n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-26T10:11:10.767739Z","iopub.execute_input":"2022-12-26T10:11:10.768133Z","iopub.status.idle":"2022-12-26T10:11:10.774607Z","shell.execute_reply.started":"2022-12-26T10:11:10.768097Z","shell.execute_reply":"2022-12-26T10:11:10.773507Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%cd /\n%mkdir /kaggle/working/content\n%mkdir /kaggle/working/content/gdrive\n%mkdir /kaggle/working/content/gdrive/MyDrive\n%mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth\n%mkdir /kaggle/working/content/models\n%mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions\n%mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/instance_images\n%mkdir /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/Regularization_images\n%cd /kaggle/working/content\n","metadata":{"execution":{"iopub.status.busy":"2022-12-26T10:11:10.776076Z","iopub.execute_input":"2022-12-26T10:11:10.776415Z","iopub.status.idle":"2022-12-26T10:11:18.519125Z","shell.execute_reply.started":"2022-12-26T10:11:10.776381Z","shell.execute_reply":"2022-12-26T10:11:18.518020Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/\n/kaggle/working/content\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wget\n!pip install tqdm\n!pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113\n!pip install tensorflow-io-gcs-filesystem==0.21.0\n!sudo apt-get install git-lfs\n!git lfs install","metadata":{"execution":{"iopub.status.busy":"2022-12-26T10:11:18.523061Z","iopub.execute_input":"2022-12-26T10:11:18.526952Z","iopub.status.idle":"2022-12-26T10:14:20.964278Z","shell.execute_reply.started":"2022-12-26T10:11:18.526910Z","shell.execute_reply":"2022-12-26T10:14:20.963061Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=684261cbaa94d780399d194a6b8ae3356685e19f1b5cd91596910712dc1603a4\n  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\nCollecting torch==1.12.0+cu113\n  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (1837.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m720.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0mm0:02\u001b[0m\n\u001b[?25hCollecting torchvision==0.13.0+cu113\n  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (23.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==0.12.0\n  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.12.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (3.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.12.0+cu113) (4.1.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.0+cu113) (2.28.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.0+cu113) (1.21.6)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.13.0+cu113) (9.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.0+cu113) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.0+cu113) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.0+cu113) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.13.0+cu113) (2.1.0)\nInstalling collected packages: torch, torchvision, torchaudio\n  Attempting uninstall: torch\n    Found existing installation: torch 1.11.0\n    Uninstalling torch-1.11.0:\n      Successfully uninstalled torch-1.11.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.12.0\n    Uninstalling torchvision-0.12.0:\n      Successfully uninstalled torchvision-0.12.0\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 0.11.0\n    Uninstalling torchaudio-0.11.0:\n      Successfully uninstalled torchaudio-0.11.0\nSuccessfully installed torch-1.12.0+cu113 torchaudio-0.12.0+cu113 torchvision-0.13.0+cu113\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting tensorflow-io-gcs-filesystem==0.21.0\n  Downloading tensorflow_io_gcs_filesystem-0.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: tensorflow-io-gcs-filesystem\nSuccessfully installed tensorflow-io-gcs-filesystem-0.21.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following NEW packages will be installed:\n  git-lfs\n0 upgraded, 1 newly installed, 0 to remove and 93 not upgraded.\nNeed to get 3316 kB of archives.\nAfter this operation, 11.1 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 git-lfs amd64 2.9.2-1 [3316 kB]\nFetched 3316 kB in 1s (2831 kB/s)\nSelecting previously unselected package git-lfs.\n(Reading database ... 108827 files and directories currently installed.)\nPreparing to unpack .../git-lfs_2.9.2-1_amd64.deb ...\nUnpacking git-lfs (2.9.2-1) ...\nSetting up git-lfs (2.9.2-1) ...\nProcessing triggers for man-db (2.9.1-1) ...\nError: Failed to call git rev-parse --git-dir: exit status 128 \nGit LFS initialized.\n","output_type":"stream"}]},{"cell_type":"code","source":"#@markdown # Dependencies\n\nfrom subprocess import getoutput\nfrom IPython.utils import capture\nimport time\n\nwith capture.capture_output() as cap:\n    %cd /kaggle/working/content\n    !pip install -q accelerate==0.12.0\n    for i in range(1,7):\n        !wget \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dependencies/Dependencies_AUT.{i}\"\n        !mv \"Dependencies_AUT.{i}\" \"Dependencies_AUT.7z.00{i}\"\n    !7z x Dependencies_AUT.7z.001\n    time.sleep(2)\n    !cp -r /kaggle/working/content/usr/local/lib/python3.8/dist-packages /usr/local/lib/python3.8/\n    !rm -r /kaggle/working/content/usr\n    for i in range(1,7):\n        !rm \"Dependencies_AUT.7z.00{i}\"\n    !pip uninstall -y diffusers\n    !git clone --branch updt https://github.com/TheLastBen/diffusers\n    !pip install -q /kaggle/working/content/diffusers\n    s = getoutput('nvidia-smi')\n    if 'T4' in s:\n        !pip install https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.14/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl\n    elif 'P100' in s:\n        !pip install https://github.com/Isotr0py/xformers-prebuild-wheels/raw/main/P100/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl\n   \nprint('\u001b[1;32mDONE !')    ","metadata":{"execution":{"iopub.status.busy":"2022-12-26T10:14:20.966478Z","iopub.execute_input":"2022-12-26T10:14:20.966947Z","iopub.status.idle":"2022-12-26T10:15:59.345736Z","shell.execute_reply.started":"2022-12-26T10:14:20.966864Z","shell.execute_reply":"2022-12-26T10:15:59.344425Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\u001b[1;32mDONE !\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport time\nfrom IPython.display import clear_output\nimport wget\n\n#@markdown - Skip this cell if you are loading a previous session that contains a trained model.\n\n#@markdown ---\n\nModel_Version = \"1.5\" #@param [ \"1.5\", \"V2.1-512px\", \"V2.1-768px\"]\n\n#@markdown - Choose which version to finetune.\n\nwith capture.capture_output() as cap: \n  %cd /kaggle/working/content/\n\ntoken=Huggingface_Token\n\n#@markdown - Leave EMPTY if you're using the v2 model.\n#@markdown - Make sure you've accepted the terms in https://huggingface.co/runwayml/stable-diffusion-v1-5\n\n#@markdown ---\nCustom_Model_Version=\"1.5\" #@param [ \"1.5\", \"V2.1-512px\", \"V2.1-768px\"]\n#@markdown - Choose wisely!\n\nPath_to_HuggingFace= \"\" #@param {type:\"string\"}\n\n\n#@markdown - Load and finetune a model from Hugging Face, must specify if v2, use the format \"profile/model\" like : runwayml/stable-diffusion-v1-5\n\n#@markdown Or\n\nCKPT_Path = \"\" #@param {type:\"string\"}\n\n#@markdown Or\n\nCKPT_Link = \"\" #@param {type:\"string\"}\n\n#@markdown - A CKPT direct link, huggingface CKPT link or a shared CKPT from gdrive.\n\n\ndef downloadmodel():\n  token=Huggingface_Token\n  if token==\"\":\n      token=input(\"Insert your huggingface token :\")\n  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n    !rm -r /kaggle/working/content/stable-diffusion-v1-5\n  clear_output()\n\n  %cd /kaggle/working/content/\n  clear_output()\n  !mkdir /kaggle/working/content/stable-diffusion-v1-5\n  %cd /kaggle/working/content/stable-diffusion-v1-5\n  !git init\n  !git lfs install --system --skip-repo\n  !git remote add -f origin  \"https://USER:{token}@huggingface.co/runwayml/stable-diffusion-v1-5\"\n  !git config core.sparsecheckout true\n  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nmodel_index.json\" > .git/info/sparse-checkout\n  !git pull origin main\n  if os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n    !git clone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n    !mv /kaggle/working/content/stable-diffusion-v1-5/sd-vae-ft-mse /kaggle/working/content/stable-diffusion-v1-5/vae\n    !rm -r /kaggle/working/content/stable-diffusion-v1-5/.git\n    %cd /kaggle/working/content/stable-diffusion-v1-5\n    !rm model_index.json\n    time.sleep(1)    \n    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n    !sed -i 's@\"clip_sample\": false@@g' /kaggle/working/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n    !sed -i 's@\"trained_betas\": null,@\"trained_betas\": null@g' /kaggle/working/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n    !sed -i 's@\"sample_size\": 256,@\"sample_size\": 512,@g' /kaggle/working/content/stable-diffusion-v1-5/vae/config.json  \n    %cd /kaggle/working/content/    \n    clear_output()\n    print('\u001b[1;32mDONE !')\n  else:\n    while not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n         print('\u001b[1;31mMake sure you accepted the terms in https://huggingface.co/runwayml/stable-diffusion-v1-5')\n         time.sleep(5)\n\ndef newdownloadmodel():\n\n  %cd /kaggle/working/content/\n  clear_output()\n  !mkdir /kaggle/working/content/stable-diffusion-v2-768\n  %cd /kaggle/working/content/stable-diffusion-v2-768\n  !git init\n  !git lfs install --system --skip-repo\n  !git remote add -f origin  \"https://USER:{token}@huggingface.co/stabilityai/stable-diffusion-2-1\"\n  !git config core.sparsecheckout true\n  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nfeature_extractor\\nmodel_index.json\" > .git/info/sparse-checkout\n  !git pull origin main\n  clear_output()\n  print('\u001b[1;32mDONE !')\n\n\ndef newdownloadmodelb():\n\n  %cd /kaggle/working/content/\n  clear_output()\n  !mkdir /kaggle/working/content/stable-diffusion-v2-512\n  %cd /kaggle/working/content/stable-diffusion-v2-512\n  !git init\n  !git lfs install --system --skip-repo\n  !git remote add -f origin  \"https://USER:{token}@huggingface.co/stabilityai/stable-diffusion-2-1-base\"\n  !git config core.sparsecheckout true\n  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nfeature_extractor\\nmodel_index.json\" > .git/info/sparse-checkout\n  !git pull origin main\n  clear_output()\n  print('\u001b[1;32mDONE !')\n    \n\nif Path_to_HuggingFace != \"\":\n  if Custom_Model_Version=='V2.1-512px' or Custom_Model_Version=='V2.1-768px':\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom\n    clear_output()\n    %cd /kaggle/working/content/\n    clear_output()\n    !mkdir /kaggle/working/content/stable-diffusion-custom\n    %cd /kaggle/working/content/stable-diffusion-custom\n    !git init\n    !git lfs install --system --skip-repo\n    !git remote add -f origin  \"https://USER:{token}@huggingface.co/{Path_to_HuggingFace}\"\n    !git config core.sparsecheckout true\n    !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nfeature_extractor\\nmodel_index.json\" > .git/info/sparse-checkout\n    !git pull origin main\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom/.git\n      %cd /kaggle/working/content/ \n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"   \n      clear_output()\n      print('\u001b[1;32mDONE !')\n    else:\n      while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n            print('\u001b[1;31mCheck the link you provided')\n            time.sleep(5)\n  else:\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom'):\n      !rm -r /kaggle/working/content/stable-diffusion-custom\n    clear_output()\n    %cd /kaggle/working/content/\n    clear_output()\n    !mkdir /kaggle/working/content/stable-diffusion-custom\n    %cd /kaggle/working/content/stable-diffusion-custom\n    !git init\n    !git lfs install --system --skip-repo\n    !git remote add -f origin  \"https://USER:{token}@huggingface.co/{Path_to_HuggingFace}\"\n    !git config core.sparsecheckout true\n    !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nmodel_index.json\" > .git/info/sparse-checkout\n    !git pull origin main\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n      !git clone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n      !mv /kaggle/working/content/stable-diffusion-custom/sd-vae-ft-mse /kaggle/working/content/stable-diffusion-custom/vae\n      !rm -r /kaggle/working/content/stable-diffusion-custom/.git\n      %cd /kaggle/working/content/stable-diffusion-custom\n      !rm model_index.json\n      time.sleep(1)\n      wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n      !sed -i 's@\"clip_sample\": false,@@g' /kaggle/working/content/stable-diffusion-custom/scheduler/scheduler_config.json\n      !sed -i 's@\"trained_betas\": null,@\"trained_betas\": null@g' /kaggle/working/content/stable-diffusion-custom/scheduler/scheduler_config.json\n      !sed -i 's@\"sample_size\": 256,@\"sample_size\": 512,@g' /kaggle/working/content/stable-diffusion-custom/vae/config.json    \n      %cd /kaggle/working/content/ \n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"   \n      clear_output()\n      print('\u001b[1;32mDONE !')\n    else:\n      while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n            print('\u001b[1;31mCheck the link you provided')\n            time.sleep(5)    \n\nelif CKPT_Path !=\"\":\n  %cd /kaggle/working/content\n  clear_output() \n  if os.path.exists(str(CKPT_Path)):\n    if Custom_Model_Version=='1.5':\n      !wget -O refmdlz https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/refmdlz\n      !unzip -o -q refmdlz\n      !rm -f refmdlz      \n      !wget -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv1.py\n      clear_output()\n      !python /kaggle/working/content/convertodiff.py \"$CKPT_Path\" /kaggle/working/content/stable-diffusion-custom --v1\n      !rm -r /kaggle/working/content/refmdl\n    elif Custom_Model_Version=='V2.1-512px':\n      !wget -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv2.py\n      clear_output()\n      !python /kaggle/working/content/convertodiff.py \"$CKPT_Path\" /kaggle/working/content/stable-diffusion-custom --v2 --reference_model stabilityai/stable-diffusion-2-1-base\n    elif Custom_Model_Version=='V2.1-768px':\n      !wget -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv2.py\n      clear_output()\n      !python /kaggle/working/content/convertodiff.py \"$CKPT_Path\" /kaggle/working/content/stable-diffusion-custom --v2 --reference_model stabilityai/stable-diffusion-2-1\n    !rm /kaggle/working/content/convertodiff.py\n    if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n      clear_output()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"\n      print('\u001b[1;32mDONE !')\n    else:\n      !rm -r /kaggle/working/content/stable-diffusion-custom\n      while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n        print('\u001b[1;31mConversion error')\n        time.sleep(5)\n  else:\n    while not os.path.exists(str(CKPT_Path)):\n       print('\u001b[1;31mWrong path, use the colab file explorer to copy the path')\n       time.sleep(5)  \n\nelif CKPT_Link !=\"\":   \n    %cd /kaggle/working/content\n    clear_output()     \n    !gdown --fuzzy -O model.ckpt $CKPT_Link\n    clear_output() \n    if os.path.exists('/kaggle/working/content/model.ckpt'):\n      if os.path.getsize(\"/kaggle/working/content/model.ckpt\") > 1810671599:\n        if Custom_Model_Version=='1.5':\n          !wget -O refmdlz https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/refmdlz\n          !unzip -o -q refmdlz\n          !rm -f refmdlz        \n          !wget -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv1.py\n          clear_output()\n          !python /kaggle/working/content/convertodiff.py /kaggle/working/content/model.ckpt /kaggle/working/content/stable-diffusion-custom --v1\n          !rm -r /kaggle/working/content/refmdl\n        elif Custom_Model_Version=='V2.1-512px':\n          !wget -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv2.py\n          clear_output()\n          !python /kaggle/working/content/convertodiff.py /kaggle/working/content/model.ckpt /kaggle/working/content/stable-diffusion-custom --v2 --reference_model stabilityai/stable-diffusion-2-1-base\n        elif Custom_Model_Version=='V2.1-768px':\n          !wget -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv2.py\n          clear_output()\n          !python /kaggle/working/content/convertodiff.py /kaggle/working/content/model.ckpt /kaggle/working/content/stable-diffusion-custom --v2 --reference_model stabilityai/stable-diffusion-2-1\n        !rm /kaggle/working/content/convertodiff.py\n        if os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n          clear_output()\n          MODEL_NAME=\"/kaggle/working/content/stable-diffusion-custom\"\n          print('\u001b[1;32mDONE !')\n        else:\n          !rm -r /kaggle/working/content/stable-diffusion-custom\n          !rm /kaggle/working/content/model.ckpt\n          while not os.path.exists('/kaggle/working/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n            print('\u001b[1;31mConversion error')\n            time.sleep(5)\n      else:\n        while os.path.getsize('/kaggle/working/content/model.ckpt') < 1810671599:\n           print('\u001b[1;31mWrong link, check that the link is valid')\n           time.sleep(5)\n    \nelse:\n  if Model_Version==\"1.5\":\n    if not os.path.exists('/kaggle/working/content/stable-diffusion-v1-5'):\n      downloadmodel()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v1-5\"\n    else:\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v1-5\"\n      print(\"\u001b[1;32mThe v1.5 model already exists, using this model.\")\n  elif Model_Version==\"V2.1-512px\":\n    if not os.path.exists('/kaggle/working/content/stable-diffusion-v2-512'):\n      newdownloadmodelb()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-512\"\n    else:\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-512\"\n      print(\"\u001b[1;32mThe v2-512px model already exists, using this model.\")      \n  elif Model_Version==\"V2.1-768px\":\n    if not os.path.exists('/kaggle/working/content/stable-diffusion-v2-768'):   \n      newdownloadmodel()\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-768\"\n    else:\n      MODEL_NAME=\"/kaggle/working/content/stable-diffusion-v2-768\"\n      print(\"\u001b[1;32mThe v2-768px model already exists, using this model.\")    ","metadata":{"execution":{"iopub.status.busy":"2022-12-26T10:15:59.348147Z","iopub.execute_input":"2022-12-26T10:15:59.348625Z","iopub.status.idle":"2022-12-26T10:18:11.388770Z","shell.execute_reply.started":"2022-12-26T10:15:59.348580Z","shell.execute_reply":"2022-12-26T10:18:11.387015Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\u001b[1;32mDONE !\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom IPython.display import clear_output\nfrom IPython.utils import capture\nfrom os import listdir\nfrom os.path import isfile\nimport wget\nimport time\n\n#@markdown #Create/Load a Session\n\ntry:\n  MODEL_NAME\n  pass\nexcept:\n  MODEL_NAME=\"\"\n  \nPT=\"\"\n\n\nwhile Session_Name==\"\":\n  print('\u001b[1;31mInput the Session Name:') \n  Session_Name=input('')\nSession_Name=Session_Name.replace(\" \",\"_\")\n\n#@markdown - Enter the session name, it if it exists, it will load it, otherwise it'll create an new session.\n\nSession_Link_optional = \"\" #@param{type: 'string'}\n\n#@markdown - Import a session from another gdrive, the shared gdrive link must point to the specific session's folder that contains the trained CKPT, remove any intermediary CKPT if any.\n\nWORKSPACE='/kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth'\n\nif Session_Link_optional !=\"\":\n  print('\u001b[1;32mDownloading session...')\nwith capture.capture_output() as cap:\n  %cd /kaggle/working/content\n  if Session_Link_optional != \"\":\n    if not os.path.exists(str(WORKSPACE+'/Sessions')):\n      %mkdir -p $WORKSPACE'/Sessions'\n      time.sleep(1)\n    %cd $WORKSPACE'/Sessions'\n    !gdown --folder --remaining-ok -O $Session_Name  $Session_Link_optional\n    %cd $Session_Name\n    !rm -r instance_images\n    !rm -r Regularization_images\n    !unzip instance_images.zip\n    !mv *.ckpt $Session_Name\".ckpt\"\n    %cd /kaggle/working/content\n\n\nINSTANCE_NAME=Session_Name\nOUTPUT_DIR=\"/kaggle/working/content/models/\"+Session_Name\nSESSION_DIR=WORKSPACE+'/Sessions/'+Session_Name\nINSTANCE_DIR=SESSION_DIR+'/instance_images'\nMDLPTH=str(SESSION_DIR+\"/\"+Session_Name+'.ckpt')\nCLASS_DIR=SESSION_DIR+'/Regularization_images'\n\nContains_faces = \"No\" \n\nModel_Version = \"1.5\" #@param [ \"1.5\", \"V2-512px\", \"V2-768px\"]\n#@markdown - Ignore this if you're not loading a previous session that contains a trained model\n\ndef reg():\n  with capture.capture_output() as cap:\n    if Contains_faces!=\"No\":\n      if not os.path.exists(str(CLASS_DIR)):\n        %mkdir -p \"$CLASS_DIR\"\n      %cd $CLASS_DIR\n      !rm -r Women Men Mix\n      !wget -O Womenz 'https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Regularization/Women'\n      !wget -O Menz 'https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Regularization/Men'\n      !wget -O Mixz 'https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/Regularization/Mix'\n      !unzip Menz\n      !unzip Womenz\n      !unzip Mixz\n      !rm Menz Womenz Mixz\n      !find . -name \"* *\" -type f | rename 's/ /_/g'\n      %cd /kaggle/working/content               \n\n\n\nif os.path.exists(str(SESSION_DIR)):\n  mdls=[ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1]==\"ckpt\"]\n  if not os.path.exists(MDLPTH) and '.ckpt' in str(mdls):  \n    \n    def f(n):  \n      k=0\n      for i in mdls:    \n        if k==n:    \n          !mv \"$SESSION_DIR/$i\" $MDLPTH\n        k=k+1\n\n    k=0\n    print('\u001b[1;33mNo final checkpoint model found, select which intermediary checkpoint to use, enter only the number, (000 to skip):\\n\u001b[1;34m')\n\n    for i in mdls:    \n      print(str(k)+'- '+i)\n      k=k+1\n    n=input()\n    while int(n)>k-1:\n      n=input()  \n    if n!=\"000\":\n      f(int(n))\n      print('\u001b[1;32mUsing the model '+ mdls[int(n)]+\" ...\")\n      time.sleep(2)\n    else:\n      print('\u001b[1;32mSkipping the intermediary checkpoints.')\n    del n\n\n  \nif os.path.exists(str(SESSION_DIR)) and not os.path.exists(MDLPTH):\n  print('\u001b[1;32mLoading session with no previous model, using the original model or the custom downloaded model')\n  reg()\n  if MODEL_NAME==\"\":\n    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n  else:\n    print('\u001b[1;32mSession Loaded, proceed to uploading instance images')\n\nelif os.path.exists(MDLPTH):\n  print('\u001b[1;32mSession found, loading the trained model ...')\n  reg()\n  if Model_Version=='1.5':\n    !wget -O refmdlz https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/refmdlz\n    !unzip -o -q refmdlz\n    !rm -f refmdlz   \n    !wget -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv1.py\n    clear_output()\n    print('\u001b[1;32mSession found, loading the trained model ...')\n    !python /kaggle/working/content/convertodiff.py \"$MDLPTH\" \"$OUTPUT_DIR\" --v1\n    !rm -r /kaggle/working/content/refmdl\n  elif Model_Version=='V2-512px':\n    !wget -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv2.py\n    clear_output()\n    print('\u001b[1;32mSession found, loading the trained model ...')\n    !python /kaggle/working/content/convertodiff.py \"$MDLPTH\" \"$OUTPUT_DIR\" --v2 --reference_model stabilityai/stable-diffusion-2-1-base\n  elif Model_Version=='V2-768px':\n    !wget -O convertodiff.py https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv2.py\n    clear_output()\n    print('\u001b[1;32mSession found, loading the trained model ...')\n    !python /kaggle/working/content/convertodiff.py \"$MDLPTH\" \"$OUTPUT_DIR\" --v2 --reference_model stabilityai/stable-diffusion-2-1\n  !rm /kaggle/working/content/convertodiff.py  \n  if os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n    resume=True    \n    clear_output()\n    print('\u001b[1;32mSession loaded.')\n  else:     \n    if not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n      print('\u001b[1;31mConversion error, if the error persists, remove the CKPT file from the current session folder')\n\nelif not os.path.exists(str(SESSION_DIR)):\n    %mkdir -p \"$INSTANCE_DIR\"\n    print('\u001b[1;32mCreating session...')\n    reg()\n    if MODEL_NAME==\"\":\n      print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n    else:\n      print('\u001b[1;32mSession created, proceed to uploading instance images')\n\n    \nif Contains_faces == \"Female\":\n  CLASS_DIR=CLASS_DIR+'/Women'\nif Contains_faces == \"Male\":\n  CLASS_DIR=CLASS_DIR+'/Men'\nif Contains_faces == \"Both\":\n  CLASS_DIR=CLASS_DIR+'/Mix'\n\ntry:\n  Contain_f\n  del Contain_f\nexcept:\n  pass\n\n    #@markdown \n\n    #@markdown # The most importent step is to rename the instance pictures of each subject to a unique unknown identifier, example :\n    #@markdown - If you have 30 pictures of yourself, simply select them all and rename only one to the chosen identifier for example : phtmejhn, the files would be : phtmejhn (1).jpg, phtmejhn (2).png ....etc then upload them, do the same for other people or objects with a different identifier, and that's it.\n    #@markdown - Check out this example : https://i.imgur.com/d2lD3rz.jpeg","metadata":{"execution":{"iopub.status.busy":"2022-12-26T10:18:11.392935Z","iopub.execute_input":"2022-12-26T10:18:11.393274Z","iopub.status.idle":"2022-12-26T10:18:12.979923Z","shell.execute_reply.started":"2022-12-26T10:18:11.393242Z","shell.execute_reply":"2022-12-26T10:18:12.978572Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\u001b[1;32mCreating session...\n\u001b[1;32mSession created, proceed to uploading instance images\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport shutil\nfrom PIL import Image\nfrom tqdm import tqdm\n\n#@markdown #Instance Images\n#@markdown ----\n\n#@markdown\n#@markdown - Run the cell to Upload the instance pictures.\n\nRemove_existing_instance_images= True #@param{type: 'boolean'}\n#@markdown - Uncheck the box to keep the existing instance images.\n\n\nif Remove_existing_instance_images:\n  if os.path.exists(str(INSTANCE_DIR)):\n    !rm -r \"$INSTANCE_DIR\"\n\nif not os.path.exists(str(INSTANCE_DIR)):\n  %mkdir -p \"$INSTANCE_DIR\"\n\n #@param{type: 'string'}\n\n#@markdown - If you prefer to specify directly the folder of the pictures instead of uploading, this will add the pictures to the existing (if any) instance images. Leave EMPTY to upload.\n\n\nCrop_size=int(Crop_size)\n\n#@markdown - Unless you want to crop them manually in a precise way, you don't need to crop your instance images externally.\n\nwhile IMAGES_FOLDER_OPTIONAL !=\"\" and not os.path.exists(str(IMAGES_FOLDER_OPTIONAL)):\n  print('\u001b[1;31mThe image folder specified does not exist, use the colab file explorer to copy the path :')\n  IMAGES_FOLDER_OPTIONAL=input('')\n\nif IMAGES_FOLDER_OPTIONAL!=\"\":\n  if Crop_images:\n    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      extension = filename.split(\".\")[1]\n      identifier=filename.split(\".\")[0]\n      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n      file = Image.open(IMAGES_FOLDER_OPTIONAL+\"/\"+filename)\n      width, height = file.size\n      if file.size !=(Crop_size, Crop_size):      \n        side_length = min(width, height)\n        left = (width - side_length)/2\n        top = (height - side_length)/2\n        right = (width + side_length)/2\n        bottom = (height + side_length)/2\n        image = file.crop((left, top, right, bottom))\n        image = image.resize((Crop_size, Crop_size))\n        if (extension.upper() == \"JPG\"):\n            image.save(new_path_with_file, format=\"JPEG\", quality = 100)\n        else:\n            image.save(new_path_with_file, format=extension.upper())\n      else:\n        !cp \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n\n  else:\n    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      %cp -r \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n \n  print('\\n\u001b[1;32mDone, proceed to the training cell')\n\n\nelif IMAGES_FOLDER_OPTIONAL ==\"\":\n  uploaded = files.upload()\n  if Crop_images:\n    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      shutil.move(filename, INSTANCE_DIR)\n      extension = filename.split(\".\")[1]\n      identifier=filename.split(\".\")[0]\n      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n      file = Image.open(new_path_with_file)\n      width, height = file.size\n      if file.size !=(Crop_size, Crop_size):        \n        side_length = min(width, height)\n        left = (width - side_length)/2\n        top = (height - side_length)/2\n        right = (width + side_length)/2\n        bottom = (height + side_length)/2\n        image = file.crop((left, top, right, bottom))\n        image = image.resize((Crop_size, Crop_size))\n        if (extension.upper() == \"JPG\"):\n            image.save(new_path_with_file, format=\"JPEG\", quality = 100)\n        else:\n            image.save(new_path_with_file, format=extension.upper())\n      clear_output()\n  else:\n    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n      shutil.move(filename, INSTANCE_DIR)\n      clear_output()\n\n  print('\\n\u001b[1;32mDone, proceed to the training cell')\n\nwith capture.capture_output() as cap:\n  %cd \"$INSTANCE_DIR\"\n  !find . -name \"* *\" -type f | rename 's/ /-/g'\n  %cd /kaggle/working/content\n  if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n    %rm -r INSTANCE_DIR+\"/.ipynb_checkpoints\"    \n\n  %cd $SESSION_DIR\n  !rm instance_images.zip\n  !zip -r instance_images instance_images\n  %cd /kaggle/working/content","metadata":{"execution":{"iopub.status.busy":"2022-12-26T10:18:12.982875Z","iopub.execute_input":"2022-12-26T10:18:12.983584Z","iopub.status.idle":"2022-12-26T10:20:10.743756Z","shell.execute_reply.started":"2022-12-26T10:18:12.983525Z","shell.execute_reply":"2022-12-26T10:20:10.742446Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"  |███████████████| 112/112 Uploaded","output_type":"stream"},{"name":"stdout","text":"\n\u001b[1;32mDone, proceed to the training cell\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"%pip install diffusers\"[training]\" accelerate \"transformers>=4.21.0\"\n%pip install ftfy\n%pip install bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2022-12-26T10:20:10.745614Z","iopub.execute_input":"2022-12-26T10:20:10.746035Z","iopub.status.idle":"2022-12-26T10:20:53.088858Z","shell.execute_reply.started":"2022-12-26T10:20:10.745970Z","shell.execute_reply":"2022-12-26T10:20:53.087015Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: diffusers[training] in /opt/conda/lib/python3.7/site-packages (0.9.0.dev0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.7/site-packages (0.12.0)\nCollecting transformers>=4.21.0\n  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from diffusers[training]) (9.1.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from diffusers[training]) (2021.11.10)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from diffusers[training]) (3.7.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from diffusers[training]) (4.13.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from diffusers[training]) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from diffusers[training]) (2.28.1)\nRequirement already satisfied: huggingface-hub>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from diffusers[training]) (0.10.1)\nCollecting modelcards>=0.1.4\n  Downloading modelcards-0.1.6-py3-none-any.whl (12 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from diffusers[training]) (2.1.0)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.7/site-packages (from diffusers[training]) (2.10.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from accelerate) (5.9.1)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (1.12.0+cu113)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from accelerate) (21.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from accelerate) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.21.0) (0.12.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.21.0) (4.64.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.10.0->diffusers[training]) (4.1.1)\nRequirement already satisfied: Jinja2 in /opt/conda/lib/python3.7/site-packages (from modelcards>=0.1.4->diffusers[training]) (3.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets->diffusers[training]) (0.70.13)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets->diffusers[training]) (3.0.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets->diffusers[training]) (2022.8.2)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets->diffusers[training]) (0.18.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets->diffusers[training]) (3.8.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets->diffusers[training]) (5.0.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets->diffusers[training]) (1.3.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets->diffusers[training]) (0.3.5.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->diffusers[training]) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->diffusers[training]) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->diffusers[training]) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->diffusers[training]) (3.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->diffusers[training]) (3.8.0)\nRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->diffusers[training]) (1.43.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard->diffusers[training]) (3.3.7)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->diffusers[training]) (1.8.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->diffusers[training]) (1.35.0)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard->diffusers[training]) (0.37.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard->diffusers[training]) (2.2.2)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard->diffusers[training]) (0.15.0)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->diffusers[training]) (0.6.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard->diffusers[training]) (0.4.6)\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorboard->diffusers[training]) (3.19.4)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->diffusers[training]) (59.8.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py>=0.4->tensorboard->diffusers[training]) (1.15.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->diffusers[training]) (1.7.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->diffusers[training]) (1.3.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->diffusers[training]) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->diffusers[training]) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->diffusers[training]) (21.4.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->diffusers[training]) (4.0.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->diffusers[training]) (0.13.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (4.8)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (4.2.4)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->diffusers[training]) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard->diffusers[training]) (2.1.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->diffusers[training]) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->diffusers[training]) (2022.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->diffusers[training]) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->diffusers[training]) (3.2.0)\nInstalling collected packages: transformers, modelcards\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.20.1\n    Uninstalling transformers-4.20.1:\n      Successfully uninstalled transformers-4.20.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.10.1 requires transformers<4.21,>=4.1, but you have transformers 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed modelcards-0.1.6 transformers-4.25.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nCollecting ftfy\n  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy) (0.2.5)\nInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.1.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.35.4-py3-none-any.whl (62.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.35.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"#@markdown ---\n#@markdown #Start DreamBooth\n#@markdown ---\nimport ftfy\nimport os\nfrom subprocess import getoutput\nfrom IPython.display import HTML\nfrom IPython.display import clear_output\nimport time\nimport random\n\nResume_Training = True #@param {type:\"boolean\"}\n\ntry:\n   resume\n   if resume and not Resume_Training:\n     print('\u001b[1;31mOverwrite your previously trained model ?, answering \"yes\" will train a new model, answering \"no\" will resume the training of the previous model?  yes or no ?\u001b[0m')\n     while True:\n        ansres=input('')\n        if ansres=='no':\n          Resume_Training = True\n          del ansres\n          break\n        elif ansres=='yes':\n          Resume_Training = False\n          resume= False\n          break\nexcept:\n  pass\n\nwhile not Resume_Training and MODEL_NAME==\"\":\n  print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n  time.sleep(5)\n\n#@markdown  - If you're not satisfied with the result, check this box, run again the cell and it will continue training the current model.\n\nMODELT_NAME=MODEL_NAME\n\n\n\nSeed='' #@param{type: 'string'}\n\n#@markdown - Leave empty for a random seed.\n\n\nRes=int(Resolution)\n\n#@markdown - Higher resolution = Higher quality, make sure the instance images are cropped to this selected size (or larger).\n\n\n\n#@markdown - Enable/disable half-precision, disabling it will double the training time and produce 4GB-5.2GB checkpoints.\n\n#GC= \"\"\n#if Resolution!=\"512\":\nGC= \"--gradient_checkpointing\"\n\nif Seed =='' or Seed=='0':\n  Seed=random.randint(1, 999999)\nelse:\n  Seed=int(Seed)\n\nif fp16:\n  prec=\"fp16\"\nelse:\n  prec=\"no\"\n\ns = getoutput('nvidia-smi')\nif 'A100' in s:\n  precision=\"no\"\n  GC= \"\"\nelse:\n  precision=prec\n\nresuming=\"\"\nif Resume_Training and os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n  MODELT_NAME=OUTPUT_DIR\n  print('\u001b[1;32mResuming Training...\u001b[0m')\n  resuming=\"Yes\"\nelif Resume_Training and not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n  print('\u001b[1;31mPrevious model not found, training a new model...\u001b[0m')\n  MODELT_NAME=MODEL_NAME\n  while MODEL_NAME==\"\":\n    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n    time.sleep(5)\n\nV2=False\nif os.path.getsize(MODELT_NAME+\"/text_encoder/pytorch_model.bin\") > 670901463:\n  V2=True\n\n#@markdown ---------------------------\n\ntry:\n   Contain_f\n   pass\nexcept:\n   Contain_f=Contains_faces\n\n\n\nif Train_text_encoder_for>=100:\n  stptxt=Training_Steps\nelif Train_text_encoder_for==0:\n  Enable_text_encoder_training= False\n  stptxt=10\nelse:\n  stptxt=int((Training_Steps*Train_text_encoder_for)/100)\n\nif not Enable_text_encoder_training:\n  Contains_faces=\"No\"\nelse:\n   Contains_faces=Contain_f\n\nif Enable_text_encoder_training:\n  Textenc=\"--train_text_encoder\"\nelse:\n  Textenc=\"\"\n\n#@markdown ---------------------------\n\nif Save_Checkpoint_Every==None:\n  Save_Checkpoint_Every=1\n#@markdown - Minimum 200 steps between each save.\nstp=0\n\nif Start_saving_from_the_step==None:\n    Start_saving_from_the_step=0\nif (Start_saving_from_the_step < 200):\n  Start_saving_from_the_step=Save_Checkpoint_Every\nstpsv=Start_saving_from_the_step\nif Save_Checkpoint_Every_n_Steps:\n  stp=Save_Checkpoint_Every\n#@markdown - Start saving intermediary checkpoints from this step.\n\n\n\n#@markdown - Auto-disconnect from google colab after the training to avoid wasting compute units.\n\ndef txtenc_train(MODELT_NAME, INSTANCE_DIR, CLASS_DIR, OUTPUT_DIR, PT, Seed, precision, GC, Training_Steps):\n    print('\u001b[1;33mTraining the text encoder with regularization...\u001b[0m')\n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    --image_captions_filename \\\n    --train_text_encoder \\\n    --dump_only_text_encoder \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --class_data_dir=\"$CLASS_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --with_prior_preservation --prior_loss_weight=1.0 \\\n    --instance_prompt=\"$PT\"\\\n    --seed=$Seed \\\n    --resolution=512 \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 $GC \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps \\\n    --num_class_images=200\n\ndef unet_train(SESSION_DIR, stpsv, stp, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, GC, Training_Steps):\n    clear_output()\n    if resuming==\"Yes\":\n      print('\u001b[1;32mResuming Training...\u001b[0m')\n    print('\u001b[1;33mTraining the UNet...\u001b[0m')\n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    --image_captions_filename \\\n    --train_only_unet \\\n    --Session_dir=$SESSION_DIR \\\n    --save_starting_step=$stpsv \\\n    --save_n_steps=$stp \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=$Res \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 $GC \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\n\ndef train_only_textenc(MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps):\n    print('\u001b[1;32mV2 + Standard GPU detected.\u001b[0m')\n    print('\u001b[1;33mTraining the text encoder...\u001b[0m')\n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    --image_captions_filename \\\n    --train_text_encoder \\\n    --dump_only_text_encoder \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=512 \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 --gradient_checkpointing \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\ndef train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps):\n    clear_output()\n    if resuming==\"Yes\":\n      print('\u001b[1;32mResuming Training...\u001b[0m')    \n    print('\u001b[1;33mTraining the UNet...\u001b[0m')\n    !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    --image_captions_filename \\\n    --train_only_unet \\\n    --save_starting_step=$stpsv \\\n    --save_n_steps=$stp \\\n    --Session_dir=$SESSION_DIR \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=$Res \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 --gradient_checkpointing \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\nif Contains_faces!=\"No\":  \n  if Enable_text_encoder_training :\n    txtenc_train(MODELT_NAME, INSTANCE_DIR, CLASS_DIR, OUTPUT_DIR, PT, Seed, precision, GC, Training_Steps=stptxt)\n  unet_train(SESSION_DIR, stpsv, stp, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, GC, Training_Steps)\n\nelif V2 and Resolution!=\"512\" and not(\"A100\" in s):\n    if Enable_text_encoder_training :\n      train_only_textenc(MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps=stptxt)\n    train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps)\n    \n\nelse:\n  !accelerate launch /kaggle/working/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n    $Textenc \\\n    --image_captions_filename \\\n    --save_starting_step=$stpsv \\\n    --stop_text_encoder_training=$stptxt \\\n    --save_n_steps=$stp \\\n    --Session_dir=$SESSION_DIR \\\n    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n    --instance_data_dir=\"$INSTANCE_DIR\" \\\n    --output_dir=\"$OUTPUT_DIR\" \\\n    --instance_prompt=\"$PT\" \\\n    --seed=$Seed \\\n    --resolution=$Res \\\n    --mixed_precision=$precision \\\n    --train_batch_size=1 \\\n    --gradient_accumulation_steps=1 $GC \\\n    --use_8bit_adam \\\n    --learning_rate=2e-6 \\\n    --lr_scheduler=\"polynomial\" \\\n    --lr_warmup_steps=0 \\\n    --max_train_steps=$Training_Steps\n\n\nif os.path.exists('/kaggle/working/content/models/'+INSTANCE_NAME+'/unet/diffusion_pytorch_model.bin'):\n  prc=\"--fp16\" if precision==\"fp16\" else \"\"\n  %cd /kaggle/working/content\n  if V2:\n    !python /kaggle/working/content/diffusers/scripts/convertosdv2.py $prc $OUTPUT_DIR $SESSION_DIR/$Session_Name\".ckpt\"\n    clear_output()\n    if os.path.exists(SESSION_DIR+\"/\"+INSTANCE_NAME+'.ckpt'):\n      clear_output()\n      print(\"\u001b[1;32mDONE, the CKPT model is in your Gdrive in the sessions folder\")\n      time.sleep(2)\n      if Disconnect_after_training :\n        runtime.unassign()      \n    else:\n      print(\"\u001b[1;31mSomething went wrong\")     \n  else:  \n    !wget -O convertosd.py https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/convertosd.py\n    clear_output()\n    if precision==\"no\":\n      !sed -i '226s@.*@@' /kaggle/working/content/convertosd.py\n    !sed -i '201s@.*@    model_path = \"{OUTPUT_DIR}\"@' /kaggle/working/content/convertosd.py\n    !sed -i '202s@.*@    checkpoint_path= \"{SESSION_DIR}/{Session_Name}.ckpt\"@' /kaggle/working/content/convertosd.py\n    !python /kaggle/working/content/convertosd.py\n    clear_output()\n    if os.path.exists(SESSION_DIR+\"/\"+INSTANCE_NAME+'.ckpt'):      \n      print(\"\u001b[1;32mDONE, the CKPT model is in your Gdrive in the sessions folder\")\n      time.sleep(2)\n      if Disconnect_after_training :\n        runtime.unassign()\n    else:\n      print(\"\u001b[1;31mSomething went wrong\")\n    \nelse:\n  print(\"\u001b[1;31mSomething went wrong\")","metadata":{"execution":{"iopub.status.busy":"2022-12-26T10:20:53.093374Z","iopub.execute_input":"2022-12-26T10:20:53.093711Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[1;31mPrevious model not found, training a new model...\u001b[0m\nProgress:|█                        |:   5%| | 550/11000 [21:36<6:50:56,  2.36s/i \u001b[0;32m Freezing the text_encoder ... \u001b[0m32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m\nProgress:|██                       |:  10%| | 1082/11000 [42:33<6:30:33,  2.36s/ \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m \u001b[0;32msdttdt \u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/\n!mv /kaggle/working/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/sketchart5/sketchart5.ckpt /kaggle/working\nfrom IPython.display import FileLink\nFileLink(r'sketchart5.ckpt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}